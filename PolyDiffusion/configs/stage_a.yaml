# Tokenization configuration
tokenization:
  method: "character"  # Options: "character", "atom_regex", "safe"
  vocab_path: ""  # Leave blank to auto-build from data

  # Vocabulary building parameters (used when vocab_path doesn't exist)
  build:
    min_freq: 1
    max_size: null  # No limit on vocabulary size
    vocab_limit_samples: null  # Number of samples to use for building vocab

  # SAFE-specific parameters (only used if method is "safe")
  safe:
    fragment_type: "brics"  # BRICS fragmentation
    min_fragment_size: 1

# Backward compatibility: old configs without tokenization section default to character
vocab_path: ""  # Leave blank to auto-build from data
model_config: PolyDiffusion/configs/model_base.yaml

# Results directory for checkpoints
results_dir: Results/stage_a

data:
  path: Data/Small_molecules_filtered_3.gz
  format: csv
  delimiter: "\t"
  fieldnames:
    - index
    - SMILES
  has_header: false
  limit: 1000000  # Use full dataset, or set integer to use subset
  shuffle: true
  cache_in_memory: true
  seed: 42

training:
  batch_size: 512          # effective batch size (micro_batch_size * grad_accum_steps)
  micro_batch_size: 512     # actual per-GPU batch size to fit memory
  grad_accum_steps: 1        # accumulate gradients to reach effective batch size
  lr: 3.0e-4
  lr_min: 1.0e-5  # Minimum LR for cosine annealing scheduler
  weight_decay: 0.02
  steps: 1000
  log_interval: 1000
  save_interval: 1000  # Save checkpoint every 1000 steps
  max_grad_norm: 1.0  # Gradient clipping
  num_workers: 32
  pin_memory: true
  use_amp: true

loss:
 

# Optional: Resume from checkpoint if training interrupted
# resume_checkpoint: Results/stage_a/checkpoint_step_500.pt
