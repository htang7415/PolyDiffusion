vocab_path: PolyDiffusion/vocab.txt
model_config: PolyDiffusion/configs/model_base.yaml

# Results directory for checkpoints
results_dir: Results/stage_a

data:
  path: Data/Small_molecules_filtered_2.gz
  format: csv
  delimiter: "\t"
  fieldnames:
    - index
    - SMILES
  has_header: false
  limit: 1000000  # Use full dataset, or set integer to use subset
  shuffle: true
  cache_in_memory: true
  seed: 42

training:
  batch_size: 2048          # effective batch size (micro_batch_size * grad_accum_steps)
  micro_batch_size: 256     # actual per-GPU batch size to fit memory
  grad_accum_steps: 8        # accumulate gradients to reach effective batch size
  lr: 3.0e-4
  lr_min: 1.0e-6  # Minimum LR for cosine annealing scheduler
  weight_decay: 0.02
  steps: 100000
  log_interval: 1000
  save_interval: 10000  # Save checkpoint every 1000 steps
  max_grad_norm: 1.0  # Gradient clipping
  num_workers: 32
  pin_memory: true
  use_amp: true

loss:
  lambda_syn: 0.5

# Optional: Resume from checkpoint if training interrupted
# resume_checkpoint: Results/stage_a/checkpoint_step_500.pt
