vocab_path: PolyDiffusion/vocab_stage_bc.txt
model_config: PolyDiffusion/configs/model_base.yaml

initialization:
  mode: scratch  # options: stage_a, scratch
  checkpoint: Results/stage_a/best_model.pt
  vocab_path: PolyDiffusion/vocab.txt   # Stage A vocabulary for weight remapping
  freeze_backbone: false        # true = freeze transformer backbone, train heads only
  reuse_token_embeddings: true  # copy shared tokens from Stage A embedding matrix
  reuse_output_head: true       # copy decoder logits for overlapping tokens

# Results directory for checkpoints
results_dir: Results/stage_b

data:
  path: Data/PI1M_filtered.csv
  format: csv
  delimiter: "\t"
  has_header: true
  limit: null
  shuffle: true
  cache_in_memory: true
  seed: 42

training:
  batch_size: 2048          # effective batch size (micro_batch_size * grad_accum_steps)
  micro_batch_size: 256     # per-device batch size to control GPU memory
  grad_accum_steps: 4      # accumulate gradients to reach effective batch size
  lr: 2.0e-4
  lr_min: 1.0e-6
  weight_decay: 0.01
  steps: 10000
  log_interval: 500
  save_interval: 2000  # Save checkpoint every 250 steps
  max_grad_norm: 1.0
  num_workers: 32
  pin_memory: true
  use_amp: true

loss:
  lambda_syn: 0.0  # disable synthesis regression when SA Score is unused
  lambda_gram: 0.1

# Optional: Resume from checkpoint if training interrupted
# resume_checkpoint: Results/stage_b/checkpoint_step_250.pt
